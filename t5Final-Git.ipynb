{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(os.getcwd())+\"/cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 12 19:44:15 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:18:00.0 Off |                    0 |\r\n",
      "| N/A   32C    P0    42W / 300W |      0MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    40W / 300W |      0MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:AF:00.0 Off |                    0 |\r\n",
      "| N/A   32C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package punkt to /home/nafis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(accelerator=None, accelerators='dp', accumulate_grad_batches=1, adam_epsilon=1e-08, amp_backend='native', amp_level='O2', augment=1, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, data_dir='', default_root_dir=None, deterministic=False, devices=None, distributed_backend=None, eval_batch_size=128, fast_dev_run=False, flush_logs_every_n_steps=100, fp_16=0, gpus=None, gradient_accumulation_steps=2, gradient_clip_algorithm='norm', gradient_clip_val=0.0, input='essay', ipus=None, label='score_evidence', learning_rate=0.0001, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, max_epochs=None, max_grad_norm=1.0, max_seq_length=512, max_steps=None, max_time=None, min_epochs=None, min_steps=None, model_name='google/t5-v1_1-large', model_name_or_path='google/t5-v1_1-large', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', n_gpu=-1, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_train_epochs=12, num_workers=28, opt_level='O1', optimizer='AdamW', output_dir='/home/nafis/projects/def-kevinlb/nafis/t5/results', overfit_batches=0.0, path='/home/nafis/projects/def-kevinlb/nafis', plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, seed=42, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test_path='/project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321', tokenizer_name_or_path='google/t5-v1_1-large', tpu_cores=None, track_grad_norm=-1, train_batch_size=16, train_path='/project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321', truncated_bptt_steps=None, val_check_interval=1.0, val_path='/project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321', warmup_steps=0, weight_decay=0.0, weights_save_path=None, weights_summary='top')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1212 19:45:38.149128 47994523938368 jupyter.py:137] Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msharar123\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">google/t5-v1_1-largescore_evidence</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sharar123/score_evidence\" target=\"_blank\">https://wandb.ai/sharar123/score_evidence</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sharar123/score_evidence/runs/2lm8ptqu\" target=\"_blank\">https://wandb.ai/sharar123/score_evidence/runs/2lm8ptqu</a><br/>\n",
       "                Run data is saved locally in <code>/project/6001769/nafis/t5/wandb/run-20211212_194538-2lm8ptqu</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /home/nafis/projects/def-kevinlb/nafis/t5/results exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hparams:\n",
      "\"accelerator\":                       None\n",
      "\"accelerators\":                      dp\n",
      "\"accumulate_grad_batches\":           1\n",
      "\"adam_epsilon\":                      1e-08\n",
      "\"amp_backend\":                       native\n",
      "\"amp_level\":                         O2\n",
      "\"augment\":                           1\n",
      "\"auto_lr_find\":                      False\n",
      "\"auto_scale_batch_size\":             False\n",
      "\"auto_select_gpus\":                  False\n",
      "\"benchmark\":                         False\n",
      "\"check_val_every_n_epoch\":           1\n",
      "\"checkpoint_callback\":               True\n",
      "\"data_dir\":                          \n",
      "\"default_root_dir\":                  None\n",
      "\"deterministic\":                     False\n",
      "\"devices\":                           None\n",
      "\"distributed_backend\":               None\n",
      "\"eval_batch_size\":                   128\n",
      "\"fast_dev_run\":                      False\n",
      "\"flush_logs_every_n_steps\":          100\n",
      "\"fp_16\":                             0\n",
      "\"gpus\":                              None\n",
      "\"gradient_accumulation_steps\":       2\n",
      "\"gradient_clip_algorithm\":           norm\n",
      "\"gradient_clip_val\":                 0.0\n",
      "\"input\":                             essay\n",
      "\"ipus\":                              None\n",
      "\"label\":                             score_evidence\n",
      "\"learning_rate\":                     0.0001\n",
      "\"limit_predict_batches\":             1.0\n",
      "\"limit_test_batches\":                1.0\n",
      "\"limit_train_batches\":               1.0\n",
      "\"limit_val_batches\":                 1.0\n",
      "\"log_every_n_steps\":                 50\n",
      "\"log_gpu_memory\":                    None\n",
      "\"logger\":                            True\n",
      "\"max_epochs\":                        None\n",
      "\"max_grad_norm\":                     1.0\n",
      "\"max_seq_length\":                    512\n",
      "\"max_steps\":                         None\n",
      "\"max_time\":                          None\n",
      "\"min_epochs\":                        None\n",
      "\"min_steps\":                         None\n",
      "\"model_name\":                        google/t5-v1_1-large\n",
      "\"model_name_or_path\":                google/t5-v1_1-large\n",
      "\"move_metrics_to_cpu\":               False\n",
      "\"multiple_trainloader_mode\":         max_size_cycle\n",
      "\"n_gpu\":                             -1\n",
      "\"num_nodes\":                         1\n",
      "\"num_processes\":                     1\n",
      "\"num_sanity_val_steps\":              2\n",
      "\"num_train_epochs\":                  12\n",
      "\"num_workers\":                       28\n",
      "\"opt_level\":                         O1\n",
      "\"optimizer\":                         AdamW\n",
      "\"output_dir\":                        /home/nafis/projects/def-kevinlb/nafis/t5/results\n",
      "\"overfit_batches\":                   0.0\n",
      "\"path\":                              /home/nafis/projects/def-kevinlb/nafis\n",
      "\"plugins\":                           None\n",
      "\"precision\":                         32\n",
      "\"prepare_data_per_node\":             True\n",
      "\"process_position\":                  0\n",
      "\"profiler\":                          None\n",
      "\"progress_bar_refresh_rate\":         None\n",
      "\"reload_dataloaders_every_epoch\":    False\n",
      "\"reload_dataloaders_every_n_epochs\": 0\n",
      "\"replace_sampler_ddp\":               True\n",
      "\"resume_from_checkpoint\":            None\n",
      "\"seed\":                              42\n",
      "\"stochastic_weight_avg\":             False\n",
      "\"sync_batchnorm\":                    False\n",
      "\"terminate_on_nan\":                  False\n",
      "\"test_path\":                         /project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321\n",
      "\"tokenizer_name_or_path\":            google/t5-v1_1-large\n",
      "\"tpu_cores\":                         None\n",
      "\"track_grad_norm\":                   -1\n",
      "\"train_batch_size\":                  16\n",
      "\"train_path\":                        /project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321\n",
      "\"truncated_bptt_steps\":              None\n",
      "\"val_check_interval\":                1.0\n",
      "\"val_path\":                          /project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321\n",
      "\"warmup_steps\":                      0\n",
      "\"weight_decay\":                      0.0\n",
      "\"weights_save_path\":                 None\n",
      "\"weights_summary\":                   top\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/pytorch_lightning/trainer/configuration_validator.py:86: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding`LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch(rather, they are called on every optimization step).\n",
      "  \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name      | Type                       | Params\n",
      "---------------------------------------------------------\n",
      "0 | model     | T5ForConditionalGeneration | 737 M \n",
      "1 | train_acc | Accuracy                   | 0     \n",
      "2 | val_acc   | Accuracy                   | 0     \n",
      "3 | train_qwk | CohenKappa                 | 0     \n",
      "4 | val_qwk   | CohenKappa                 | 0     \n",
      "---------------------------------------------------------\n",
      "737 M     Trainable params\n",
      "0         Non-trainable params\n",
      "737 M     Total params\n",
      "2,950.672 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a368b9a68aa946e4944a5929394222f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation 1017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.0 val-loss :  tensor(19.6781, device='cuda:0')\n",
      "Step end qwk : 0.0 val-loss :  tensor(19.3552, device='cuda:0')\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b488c48f4148f09a4af348ce3e50e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65ca612e1b5498fa197bca605aedef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.0 val-loss :  tensor(0.7944, device='cuda:0')\n",
      "Step end qwk : 0.0 val-loss :  tensor(0.7678, device='cuda:0')\n",
      "Step end qwk : 0.0 val-loss :  tensor(0.8275, device='cuda:0')\n",
      "Step end qwk : 0.0 val-loss :  tensor(0.8067, device='cuda:0')\n",
      "Step end qwk : 0.0 val-loss :  tensor(0.7877, device='cuda:0')\n",
      "Step end qwk : 0.0 val-loss :  tensor(0.7426, device='cuda:0')\n",
      "Step end qwk : 0.0 val-loss :  tensor(0.7440, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2007ccc7b54c4f46b373a6efc3735cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.03507046869878727 val-loss :  tensor(0.7691, device='cuda:0')\n",
      "Step end qwk : -0.0024330900243310083 val-loss :  tensor(0.7396, device='cuda:0')\n",
      "Step end qwk : 0.03928774781303379 val-loss :  tensor(0.8377, device='cuda:0')\n",
      "Step end qwk : 0.10960799385088393 val-loss :  tensor(0.7125, device='cuda:0')\n",
      "Step end qwk : 0.04044495496040146 val-loss :  tensor(0.7469, device='cuda:0')\n",
      "Step end qwk : 0.06847171140151809 val-loss :  tensor(0.7069, device='cuda:0')\n",
      "Step end qwk : 0.07942154512412036 val-loss :  tensor(0.7057, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba5ad5536bc4ced9115f99a9d2b5b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.03511208093632212 val-loss :  tensor(0.7660, device='cuda:0')\n",
      "Step end qwk : -0.021485573971761873 val-loss :  tensor(0.7460, device='cuda:0')\n",
      "Step end qwk : 0.11158387405595149 val-loss :  tensor(0.8231, device='cuda:0')\n",
      "Step end qwk : 0.13620237544346758 val-loss :  tensor(0.6981, device='cuda:0')\n",
      "Step end qwk : 0.14724838869608325 val-loss :  tensor(0.7193, device='cuda:0')\n",
      "Step end qwk : 0.09752639042067124 val-loss :  tensor(0.7019, device='cuda:0')\n",
      "Step end qwk : 0.0927962819519752 val-loss :  tensor(0.6953, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18403e5c99484f49aba1ad788a82f3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.05040650406504066 val-loss :  tensor(0.7643, device='cuda:0')\n",
      "Step end qwk : -0.024630541871921263 val-loss :  tensor(0.7568, device='cuda:0')\n",
      "Step end qwk : 0.1459666393218485 val-loss :  tensor(0.8116, device='cuda:0')\n",
      "Step end qwk : 0.11614555046939246 val-loss :  tensor(0.6936, device='cuda:0')\n",
      "Step end qwk : 0.1751731213131572 val-loss :  tensor(0.7096, device='cuda:0')\n",
      "Step end qwk : 0.08278622898318655 val-loss :  tensor(0.7163, device='cuda:0')\n",
      "Step end qwk : 0.06112469437652812 val-loss :  tensor(0.6994, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb58ee35a84e4640af8487af3cd45a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.16042146395924917 val-loss :  tensor(0.7573, device='cuda:0')\n",
      "Step end qwk : -0.024039962535123305 val-loss :  tensor(0.7623, device='cuda:0')\n",
      "Step end qwk : 0.1499118165784833 val-loss :  tensor(0.8259, device='cuda:0')\n",
      "Step end qwk : 0.11159635030804027 val-loss :  tensor(0.6922, device='cuda:0')\n",
      "Step end qwk : 0.22745660968289183 val-loss :  tensor(0.7222, device='cuda:0')\n",
      "Step end qwk : 0.09341104095839403 val-loss :  tensor(0.7254, device='cuda:0')\n",
      "Step end qwk : 0.054863040889241765 val-loss :  tensor(0.6989, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d36f1a911d54193829f83bd6687e702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.13149060781372757 val-loss :  tensor(0.7659, device='cuda:0')\n",
      "Step end qwk : 0.017499204581609917 val-loss :  tensor(0.7829, device='cuda:0')\n",
      "Step end qwk : 0.1252430515841244 val-loss :  tensor(0.8151, device='cuda:0')\n",
      "Step end qwk : 0.2240042974445553 val-loss :  tensor(0.6945, device='cuda:0')\n",
      "Step end qwk : 0.22521303936155823 val-loss :  tensor(0.7118, device='cuda:0')\n",
      "Step end qwk : 0.0883932931792284 val-loss :  tensor(0.7288, device='cuda:0')\n",
      "Step end qwk : 0.07482546201232032 val-loss :  tensor(0.6955, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef02831fd2548019141a5cffb936d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.2148391641230336 val-loss :  tensor(0.8034, device='cuda:0')\n",
      "Step end qwk : 0.024125779343995624 val-loss :  tensor(0.8460, device='cuda:0')\n",
      "Step end qwk : 0.25209210200544685 val-loss :  tensor(0.8632, device='cuda:0')\n",
      "Step end qwk : 0.30494669227203497 val-loss :  tensor(0.6903, device='cuda:0')\n",
      "Step end qwk : 0.27296220523312154 val-loss :  tensor(0.7518, device='cuda:0')\n",
      "Step end qwk : 0.19859303552585295 val-loss :  tensor(0.7762, device='cuda:0')\n",
      "Step end qwk : 0.21037504381352967 val-loss :  tensor(0.7382, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbcf285a5424e9281146755641a5c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.15974474317397214 val-loss :  tensor(0.8034, device='cuda:0')\n",
      "Step end qwk : 0.05249547452805792 val-loss :  tensor(0.8135, device='cuda:0')\n",
      "Step end qwk : 0.28135527022408824 val-loss :  tensor(0.8469, device='cuda:0')\n",
      "Step end qwk : 0.2627381479840496 val-loss :  tensor(0.7004, device='cuda:0')\n",
      "Step end qwk : 0.3018069973087274 val-loss :  tensor(0.7717, device='cuda:0')\n",
      "Step end qwk : 0.1686560876281421 val-loss :  tensor(0.7884, device='cuda:0')\n",
      "Step end qwk : 0.23370469970546803 val-loss :  tensor(0.7312, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f35697f07024a49b725112a38b5d1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.22040223927016378 val-loss :  tensor(0.8436, device='cuda:0')\n",
      "Step end qwk : 0.08028503562945366 val-loss :  tensor(0.8644, device='cuda:0')\n",
      "Step end qwk : 0.2615833637358628 val-loss :  tensor(0.8940, device='cuda:0')\n",
      "Step end qwk : 0.29941357162803683 val-loss :  tensor(0.7570, device='cuda:0')\n",
      "Step end qwk : 0.32676300873206354 val-loss :  tensor(0.8150, device='cuda:0')\n",
      "Step end qwk : 0.12677895286590934 val-loss :  tensor(0.8398, device='cuda:0')\n",
      "Step end qwk : 0.23314065510597304 val-loss :  tensor(0.7698, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b508e74bae14bfcbfd01aa9867395bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.2603135534271497 val-loss :  tensor(0.8664, device='cuda:0')\n",
      "Step end qwk : 0.04682520225545472 val-loss :  tensor(0.9330, device='cuda:0')\n",
      "Step end qwk : 0.22880592058446791 val-loss :  tensor(0.9119, device='cuda:0')\n",
      "Step end qwk : 0.3580926180651077 val-loss :  tensor(0.7599, device='cuda:0')\n",
      "Step end qwk : 0.21935548639694813 val-loss :  tensor(0.8971, device='cuda:0')\n",
      "Step end qwk : 0.08437583178067609 val-loss :  tensor(0.8666, device='cuda:0')\n",
      "Step end qwk : 0.12974721456810834 val-loss :  tensor(0.8164, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bcf4a73a3a419a9f367591e0bf6012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.2797265316855114 val-loss :  tensor(0.8868, device='cuda:0')\n",
      "Step end qwk : 0.04995287464655984 val-loss :  tensor(0.9665, device='cuda:0')\n",
      "Step end qwk : 0.22113836537550902 val-loss :  tensor(0.9431, device='cuda:0')\n",
      "Step end qwk : 0.3699693091667149 val-loss :  tensor(0.7513, device='cuda:0')\n",
      "Step end qwk : 0.21657250470809797 val-loss :  tensor(0.9168, device='cuda:0')\n",
      "Step end qwk : 0.09453115086917907 val-loss :  tensor(0.8947, device='cuda:0')\n",
      "Step end qwk : 0.1410416295504484 val-loss :  tensor(0.8312, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e50aa5ef3c430886973b4cd4861149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.21345859733449857 val-loss :  tensor(0.9440, device='cuda:0')\n",
      "Step end qwk : -0.008064516129032251 val-loss :  tensor(0.9823, device='cuda:0')\n",
      "Step end qwk : 0.1871118374970815 val-loss :  tensor(0.9990, device='cuda:0')\n",
      "Step end qwk : 0.3331689272503082 val-loss :  tensor(0.7511, device='cuda:0')\n",
      "Step end qwk : 0.16828050147793294 val-loss :  tensor(0.9676, device='cuda:0')\n",
      "Step end qwk : 0.09433962264150941 val-loss :  tensor(0.9110, device='cuda:0')\n",
      "Step end qwk : 0.16094032549728754 val-loss :  tensor(0.8662, device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install pytorch_lightning\n",
    "\n",
    "# ## T5 fine-tuning with Pytorch Lightning and Wandb\n",
    "# \n",
    "# \n",
    "#  [T5 model](https://arxiv.org/abs/1910.10683) with Huggingface's [Transformers](https://github.com/huggingface/transformers/) to solve different NLP tasks using text-2-text approach proposed in the T5 paper. \n",
    "#  \n",
    "# Transfer Learning with T5 with wandb \n",
    "# Author Nafis Abrar \n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning.metrics.functional as FM\n",
    "import torchmetrics\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import CohenKappa\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torchmetrics.functional import cohen_kappa\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    Adafactor,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5Config,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from pytorch_lightning.loggers import WandbLogger \n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from torchmetrics.functional import cohen_kappa\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# scikit-learn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, cohen_kappa_score, confusion_matrix, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "dataSeed = 12321\n",
    "train_path = \"Insert Train Folder containgin train.csv\"\n",
    "val_path = \"Insert Val Folder path containg val.csv\"\n",
    "test_path = \"Insert Test Folder path containg test.csv\"\n",
    "\n",
    "def getData(path):\n",
    "    data = pd.read_csv(os.path.join(train_path, \"train.csv\"))\n",
    "    data[args.label] = data[args.label].astype(int)\n",
    "    data[\"label\"] = data[args.label]\n",
    "    data = data.sample(frac=1,random_state=seed).reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def getValidationData(path):\n",
    "    validation = pd.read_csv(val_path+\"/val.csv\")\n",
    "    validation[args.label] = validation[args.label].astype(int)\n",
    "    validation[\"label\"] = validation[args.label]\n",
    "    validation = validation.sample(frac=1,random_state=seed).reset_index(drop=True)   \n",
    "    return validation\n",
    "    \n",
    "    \n",
    "def getTestData(path):\n",
    "    test =  pd.read_csv(test_path+\"/test.csv\")\n",
    "    test[args.label] = test[args.label].astype(int)\n",
    "    test[\"label\"] = test[args.label]\n",
    "    test = test.sample(frac=1,random_state=seed).reset_index(drop=True)\n",
    "    return test\n",
    "        \n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "\n",
    "# ## Model\n",
    "# \n",
    "# We'll be using the awesome [pytorch-lightning](https://github.com/PytorchLightning/pytorch-lightning) library for training. Most of the below code is adapted from here https://github.com/huggingface/transformers/blob/master/examples/lightning_base.py\n",
    "# \n",
    "# The trainer is generic and can be used for any text-2-text task. You'll just need to change the dataset. Rest of the code will stay unchanged for all the tasks.\n",
    "# \n",
    "# This is the most intresting and powrfull thing about the text-2-text format. You can fine-tune the model on variety of NLP tasks by just formulating the problem in text-2-text setting. No need to change hyperparameters, learning rate, optimizer or loss function. Just plug in your dataset and you are ready to go!\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "class T5Custom(pl.LightningModule):\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"T5Custom\")\n",
    "        parser.add_argument(\"--model_name\", type=str, default=\"google/t5-v1_1-large\")\n",
    "        parser.add_argument(\"--model_name_or_path\", type=str, default=\"google/t5-v1_1-large\")\n",
    "        parser.add_argument(\"--optimizer\", type=str, default=\"AdamW\")\n",
    "        parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=\"google/t5-v1_1-large\")\n",
    "        parser.add_argument(\"--n_gpu\", type=int, default=-1)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
    "        parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
    "        parser.add_argument(\"--adam_epsilon\", type=float, default=1e-8)\n",
    "        parser.add_argument(\"--warmup_steps\", type=int, default=0)\n",
    "        parser.add_argument(\"--num_train_epochs\", type=int, default=12)\n",
    "        parser.add_argument(\"--train_batch_size\", type=int, default=16)\n",
    "        parser.add_argument(\"--eval_batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=2)\n",
    "        parser.add_argument(\"--fp_16\", type=int, default=0)\n",
    "        parser.add_argument(\"--opt_level\", type=str, default=\"O1\")\n",
    "        parser.add_argument(\"--max_grad_norm\", type=float, default=1.0)\n",
    "        parser.add_argument(\"--max_seq_length\", type=int, default=512)\n",
    "        parser.add_argument(\"--data_dir\", type=str, default=\"\")\n",
    "        return parent_parser\n",
    "        \n",
    "        \n",
    "    def __init__(self, hparams,*args, **kwargs):\n",
    "        super().__init__()\n",
    "        if type(hparams) is dict: hparams = argparse.Namespace(**hparams)\n",
    "        self.save_hyperparameters(hparams) \n",
    "        print(f\" hparams:\\n{self.hparams}\\n\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.train_qwk = torchmetrics.CohenKappa(num_classes=5)\n",
    "        self.val_qwk = torchmetrics.CohenKappa(num_classes=5)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _step(self, batch,return_text=False):\n",
    "        labels = batch[\"target_ids\"]\n",
    "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        source_ids, source_mask, y = batch[\"source_ids\"], batch[\"source_mask\"], batch[\"target_ids\"]\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone()\n",
    "        # Change pad_token_id to -100\n",
    "        lm_labels[y[:, 1:] == pad_token_id] = -100\n",
    "        # Run forward pass and calculate loss\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )    # Only get loss from the output since that's all we need to apply our optimizer\n",
    "\n",
    "        loss = outputs[0]\n",
    "        if return_text:\n",
    "            target_text = [self.tokenizer.decode(ids) for ids in y_ids]\n",
    "            return loss, target_text\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        training_step, which takes a batch \n",
    "        and computes the loss; backprop goes through it\n",
    "        \"\"\"\n",
    "        loss = self._step(batch)\n",
    "        losses = loss.clone().detach()\n",
    "        # self.log takes a name and value for a metric. Under the hood, this will get passed to wandb.log if you're using W&B.\n",
    "        return {\"loss\": loss,\"losses\":losses}\n",
    "\n",
    "        \n",
    "    def training_epoch_end(self, outputs):\n",
    "        if outputs:\n",
    "            avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "            self.log('training-epoch-loss',avg_train_loss,on_step=False,on_epoch=True)\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        methods that trigger on each batch for a dataset: validation_step and test_step\n",
    "        \"\"\"\n",
    "        loss, target_text = self._step(batch, return_text=True)\n",
    "        preds = self.test_step(batch, batch_idx)\n",
    "        preds_text = preds[\"preds\"]\n",
    "        targets = [(int(x)) if x.isnumeric() else 0  for x in target_text ]\n",
    "        outputs = [int(x)  if x.isnumeric() else 0 for x in preds_text ]\n",
    "        return {\"val_loss\": loss,\"labels\":targets,\"pred\":outputs}\n",
    "\n",
    "    # Gathered data from all processes (per single step)\n",
    "    # Allows for accumulation so the whole data at the end of epoch\n",
    "    # takes less memory\n",
    "    def validation_step_end(self, batch_parts):\n",
    "        predictions = batch_parts[\"pred\"]\n",
    "        labels = batch_parts[\"labels\"]\n",
    "        predictions = [int(t.item()) for tensor in predictions for t in tensor]\n",
    "        outputs = [int(t.item()) for tensor in labels for t in tensor]\n",
    "        kappa = sklearn.metrics.cohen_kappa_score(outputs, predictions, weights=\"quadratic\")  \n",
    "        if math.isnan(kappa):\n",
    "            print (\"kappa nan step\")\n",
    "        #Loss from individual GPUs\n",
    "        #https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n",
    "        losses = batch_parts[\"val_loss\"]\n",
    "        total_loss = (losses[0]+losses[1]+losses[2]+losses[3])/4\n",
    "    #         self.log('valid/acc_epoch', self.val_acc(targetsToTens,outputsToTensor),on_epoch=True,sync_dist=True)\n",
    "        self.log('val-epoch-loss', total_loss, on_epoch=True,sync_dist=True)\n",
    "        self.log('val_qwk', kappa, on_epoch=True,on_step=False,sync_dist=True)\n",
    "        self.log('val_qwk_step', kappa, on_step=True,sync_dist=True)\n",
    "\n",
    "        print (\"Step end qwk :\",kappa,\"val-loss : \",total_loss)\n",
    "        cm = metrics.confusion_matrix(predictions, outputs)\n",
    "#         print (cm)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay if args.optimizer==\"AdamW\" else 0.0,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        if args.optimizer==\"AdamW\":\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon, correct_bias=False,)\n",
    "#         optimizer = AdaFactor(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon,correct_bias=False)\n",
    "        else:\n",
    "            optimizer = Adafactor(optimizer_grouped_parameters, scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3)\n",
    "#         optimizer = Adafactor(optimizer_grouped_parameters, scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    \n",
    "    def optimizer_step(self,\n",
    "                     epoch,\n",
    "                     batch_idx,\n",
    "                     optimizer,\n",
    "                     optimizer_idx,\n",
    "                     optimizer_closure=None,\n",
    "                     on_tpu=None,\n",
    "                     using_native_amp=None,\n",
    "                     using_lbfgs=None):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "        return tqdm_dict\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=False, num_workers=self.hparams.num_workers)\n",
    "        if self.hparams.gradient_accumulation_steps>1:\n",
    "            print (self.hparams.gradient_accumulation_steps)\n",
    "            t_total = (\n",
    "                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "                // self.hparams.gradient_accumulation_steps\n",
    "                * float(self.hparams.num_train_epochs)\n",
    "            )\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "            )\n",
    "        else:\n",
    "            tb_size = self.hparams.train_batch_size * max(1, self.hparams.n_gpu)\n",
    "            ab_size = self.hparams.gradient_accumulation_steps * float(self.hparams.num_train_epochs)\n",
    "            self.total_steps = (len(dataloader.dataset) // tb_size) // ab_size\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                self.opt,\n",
    "                num_warmup_steps=self.hparams.warmup_steps,\n",
    "                num_training_steps=self.total_steps,\n",
    "            )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "    # Step during testing\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Runs forward pass on test set and returns calculated loss, predictions, and targets\n",
    "        Note: this assumes that your test set has targets (doesn't have for kaggle).\n",
    "        \"\"\"\n",
    "        outputs,targets = [],[]\n",
    "        outs = self.model.generate(input_ids=batch['source_ids'], \n",
    "                                  attention_mask=batch['source_mask'], \n",
    "                                  max_length=2)\n",
    "\n",
    "        dec = [tokenizer.decode(ids) for ids in outs]\n",
    "        target = [tokenizer.decode(ids) for ids in batch[\"target_ids\"]]\n",
    "        outputs.extend(dec)\n",
    "        targets.extend(target)\n",
    "        preds = [x.split(\" \")[-1] for x in outputs]\n",
    "        targets = [x.split(\"<\")[0] for x in targets]\n",
    "        return {\"preds\": preds}\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4,drop_last=True)\n",
    "    \n",
    "\n",
    "def get_dataset(tokenizer, type_path, args):\n",
    "    return Dataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length,args=args)\n",
    "\n",
    "\n",
    "############################Dataset####################################################################\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_dir, type_path, args, max_len=512):\n",
    "        self.path = args.train_path if type_path==\"train\" else args.val_path if type_path==\"val\" else args.test_path\n",
    "        self.data_column = args.input\n",
    "        self.class_column = args.label\n",
    "        self.data = getValidationData(self.path) if type_path==\"val\" else getData(self.path) if type_path==\"train\" else getTestData(self.path)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "        src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "    def _build(self):\n",
    "        for idx in range(len(self.data)):\n",
    "            \n",
    "            input_, target = self.data.loc[idx, self.data_column], self.data.loc[idx, self.class_column]      \n",
    "            input_,target = str(input_),str(target)\n",
    "            input_ = input_ + ' </s>'\n",
    "            target = target + \" </s>\"\n",
    "\n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "              [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "              [target], max_length=2, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "wandb_logger = WandbLogger(log_model=True)  # newline 2\n",
    "logger=wandb_logger\n",
    "\n",
    "\n",
    "class wandbCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "            metrics = trainer.callback_metrics\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    wandb.log({key:metrics[key]})\n",
    "        \n",
    "\n",
    "\n",
    "# \n",
    "# Let's define the hyperparameters and other arguments. You can overide this `dict` for specific task as needed. While in most of cases you'll only need to change the `data_dir`and `output_dir`.\n",
    "# \n",
    "# Here the batch size is 8 and gradient_accumulation_steps are 16 so the effective batch size is 128\n",
    "\n",
    "\"\"\"\n",
    "################## Argparser Best Practices ########################\n",
    "\n",
    "Argparser Best Practices\n",
    "It is best practice to layer your arguments in three sections.\n",
    "\n",
    "1.Trainer args (gpus, num_nodes, etc…)\n",
    "\n",
    "2.Model specific arguments (layer_dim, num_layers, learning_rate, etc…)\n",
    "\n",
    "3.Program arguments (data_path, cluster_email, etc…)\n",
    "\n",
    "#####################################################################\n",
    "\"\"\"\n",
    "parser = ArgumentParser(description='Initiating script')\n",
    "# add PROGRAM level args\n",
    "parser.add_argument(\"--train_path\", type=str, default=train_path)\n",
    "parser.add_argument(\"--val_path\", type=str, default=val_path)\n",
    "parser.add_argument(\"--test_path\", type=str, default=test_path)\n",
    "parser.add_argument(\"--input\", type=str, default=\"insert input column name\")\n",
    "parser.add_argument(\"--label\", type=str, default=\"insert label name\")\n",
    "parser.add_argument(\"--path\", type=str, default=\"/\")\n",
    "parser.add_argument(\"--output_dir\", type=str, default=\"/\")\n",
    "parser.add_argument(\"--augment\", type=int, default=1)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=multiprocessing.cpu_count()-4)\n",
    "parser.add_argument(\"--accelerators\", type=str, default=\"dp\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "# add model specific args\n",
    "parser = T5Custom.add_model_specific_args(parser)\n",
    "# add all the available trainer options to argparse\n",
    "# ie: now --gpus --num_nodes ... --fast_dev_run all work in the cli\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "args = parser.parse_args(args=[])\n",
    "set_seed(args.seed)\n",
    "\n",
    "# add model specific args\n",
    "model_name = args.model_name\n",
    "target_col = args.label\n",
    "seed = args.seed\n",
    "augmentation = 0\n",
    "\n",
    "####################WANDB#####################################################\n",
    "wandb.login()\n",
    "wandb.init(project=args.label,name=str(args.model_name+args.label))\n",
    "wconfig = wandb.config\n",
    "###############################################################################\n",
    "os.chdir(args.path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(args.model_name)\n",
    "# ### Train\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "PATH = args.path\n",
    "\n",
    "####################CallBack#####################################################\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=args.output_dir,\n",
    "    filename=\"mta-{target_col}-{val_qwk_step}\", monitor=\"val_qwk_step\", mode=\"max\", save_top_k=1,every_n_train_steps=10,\n",
    ")\n",
    "###############################################################################\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    accelerator=args.accelerators,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    precision= 16 if args.fp_16==1 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    logger=WandbLogger(log_model=True),\n",
    "    callbacks=[checkpoint_callback,wandbCallback()],\n",
    "\n",
    ")\n",
    "\n",
    "model = T5Custom(args)\n",
    "\n",
    "# For Gradual Freezing\n",
    "\n",
    "# for name,param in model.model.named_parameters():\n",
    "#     if ('final_layer_norm' in name) or (\"decoder.block.23\" in name) or (\"lm_head.weight\") or (\"decoder.final_layer_norm.weight\") in name:\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False\n",
    "#     print (name,param.requires_grad)\n",
    "\n",
    "trainer = pl.Trainer(**train_params)\n",
    "trainer.fit(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test datasize 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/transformers/models/t5/tokenization_t5.py:191: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf772adac9e43e682a5a584cc2d6606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.2072230786170447 val-loss :  tensor(0.8824, device='cuda:0')\n",
      "Step end qwk : 0.026817804810616575 val-loss :  tensor(0.8802, device='cuda:0')\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val-epoch-loss': 0.8814563751220703,\n",
      " 'val_qwk': 0.1299065202474594,\n",
      " 'val_qwk_step': 0.1299065202474594,\n",
      " 'val_qwk_step_epoch': 0.1299065202474594}\n",
      "--------------------------------------------------------------------------------\n",
      " hparams:\n",
      "\"accelerator\":                       None\n",
      "\"accelerators\":                      dp\n",
      "\"accumulate_grad_batches\":           1\n",
      "\"adam_epsilon\":                      1e-08\n",
      "\"amp_backend\":                       native\n",
      "\"amp_level\":                         O2\n",
      "\"augment\":                           1\n",
      "\"auto_lr_find\":                      False\n",
      "\"auto_scale_batch_size\":             False\n",
      "\"auto_select_gpus\":                  False\n",
      "\"benchmark\":                         False\n",
      "\"check_val_every_n_epoch\":           1\n",
      "\"checkpoint_callback\":               True\n",
      "\"data_dir\":                          \n",
      "\"default_root_dir\":                  None\n",
      "\"deterministic\":                     False\n",
      "\"devices\":                           None\n",
      "\"distributed_backend\":               None\n",
      "\"eval_batch_size\":                   128\n",
      "\"fast_dev_run\":                      False\n",
      "\"flush_logs_every_n_steps\":          100\n",
      "\"fp_16\":                             0\n",
      "\"gpus\":                              None\n",
      "\"gradient_accumulation_steps\":       2\n",
      "\"gradient_clip_algorithm\":           norm\n",
      "\"gradient_clip_val\":                 0.0\n",
      "\"input\":                             essay\n",
      "\"ipus\":                              None\n",
      "\"label\":                             score_evidence\n",
      "\"learning_rate\":                     0.0001\n",
      "\"limit_predict_batches\":             1.0\n",
      "\"limit_test_batches\":                1.0\n",
      "\"limit_train_batches\":               1.0\n",
      "\"limit_val_batches\":                 1.0\n",
      "\"log_every_n_steps\":                 50\n",
      "\"log_gpu_memory\":                    None\n",
      "\"logger\":                            True\n",
      "\"max_epochs\":                        None\n",
      "\"max_grad_norm\":                     1.0\n",
      "\"max_seq_length\":                    512\n",
      "\"max_steps\":                         None\n",
      "\"max_time\":                          None\n",
      "\"min_epochs\":                        None\n",
      "\"min_steps\":                         None\n",
      "\"model_name\":                        google/t5-v1_1-large\n",
      "\"model_name_or_path\":                google/t5-v1_1-large\n",
      "\"move_metrics_to_cpu\":               False\n",
      "\"multiple_trainloader_mode\":         max_size_cycle\n",
      "\"n_gpu\":                             -1\n",
      "\"num_nodes\":                         1\n",
      "\"num_processes\":                     1\n",
      "\"num_sanity_val_steps\":              2\n",
      "\"num_train_epochs\":                  12\n",
      "\"num_workers\":                       28\n",
      "\"opt_level\":                         O1\n",
      "\"optimizer\":                         AdamW\n",
      "\"output_dir\":                        /home/nafis/projects/def-kevinlb/nafis/t5/results\n",
      "\"overfit_batches\":                   0.0\n",
      "\"path\":                              /home/nafis/projects/def-kevinlb/nafis\n",
      "\"plugins\":                           None\n",
      "\"precision\":                         32\n",
      "\"prepare_data_per_node\":             True\n",
      "\"process_position\":                  0\n",
      "\"profiler\":                          None\n",
      "\"progress_bar_refresh_rate\":         None\n",
      "\"reload_dataloaders_every_epoch\":    False\n",
      "\"reload_dataloaders_every_n_epochs\": 0\n",
      "\"replace_sampler_ddp\":               True\n",
      "\"resume_from_checkpoint\":            None\n",
      "\"seed\":                              42\n",
      "\"stochastic_weight_avg\":             False\n",
      "\"sync_batchnorm\":                    False\n",
      "\"terminate_on_nan\":                  False\n",
      "\"test_path\":                         /project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321\n",
      "\"tokenizer_name_or_path\":            google/t5-v1_1-large\n",
      "\"tpu_cores\":                         None\n",
      "\"track_grad_norm\":                   -1\n",
      "\"train_batch_size\":                  16\n",
      "\"train_path\":                        /project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321\n",
      "\"truncated_bptt_steps\":              None\n",
      "\"val_check_interval\":                1.0\n",
      "\"val_path\":                          /project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321\n",
      "\"warmup_steps\":                      0\n",
      "\"weight_decay\":                      0.0\n",
      "\"weights_save_path\":                 None\n",
      "\"weights_summary\":                   top\n",
      "\n",
      " hparams:\n",
      "\"accelerator\":                       None\n",
      "\"accelerators\":                      dp\n",
      "\"accumulate_grad_batches\":           1\n",
      "\"adam_epsilon\":                      1e-08\n",
      "\"amp_backend\":                       native\n",
      "\"amp_level\":                         O2\n",
      "\"augment\":                           1\n",
      "\"auto_lr_find\":                      False\n",
      "\"auto_scale_batch_size\":             False\n",
      "\"auto_select_gpus\":                  False\n",
      "\"benchmark\":                         False\n",
      "\"check_val_every_n_epoch\":           1\n",
      "\"checkpoint_callback\":               True\n",
      "\"data_dir\":                          \n",
      "\"default_root_dir\":                  None\n",
      "\"deterministic\":                     False\n",
      "\"devices\":                           None\n",
      "\"distributed_backend\":               None\n",
      "\"eval_batch_size\":                   128\n",
      "\"fast_dev_run\":                      False\n",
      "\"flush_logs_every_n_steps\":          100\n",
      "\"fp_16\":                             0\n",
      "\"gpus\":                              None\n",
      "\"gradient_accumulation_steps\":       2\n",
      "\"gradient_clip_algorithm\":           norm\n",
      "\"gradient_clip_val\":                 0.0\n",
      "\"input\":                             essay\n",
      "\"ipus\":                              None\n",
      "\"label\":                             score_evidence\n",
      "\"learning_rate\":                     0.0001\n",
      "\"limit_predict_batches\":             1.0\n",
      "\"limit_test_batches\":                1.0\n",
      "\"limit_train_batches\":               1.0\n",
      "\"limit_val_batches\":                 1.0\n",
      "\"log_every_n_steps\":                 50\n",
      "\"log_gpu_memory\":                    None\n",
      "\"logger\":                            True\n",
      "\"max_epochs\":                        None\n",
      "\"max_grad_norm\":                     1.0\n",
      "\"max_seq_length\":                    512\n",
      "\"max_steps\":                         None\n",
      "\"max_time\":                          None\n",
      "\"min_epochs\":                        None\n",
      "\"min_steps\":                         None\n",
      "\"model_name\":                        google/t5-v1_1-large\n",
      "\"model_name_or_path\":                google/t5-v1_1-large\n",
      "\"move_metrics_to_cpu\":               False\n",
      "\"multiple_trainloader_mode\":         max_size_cycle\n",
      "\"n_gpu\":                             -1\n",
      "\"num_nodes\":                         1\n",
      "\"num_processes\":                     1\n",
      "\"num_sanity_val_steps\":              2\n",
      "\"num_train_epochs\":                  12\n",
      "\"num_workers\":                       28\n",
      "\"opt_level\":                         O1\n",
      "\"optimizer\":                         AdamW\n",
      "\"output_dir\":                        /home/nafis/projects/def-kevinlb/nafis/t5/results\n",
      "\"overfit_batches\":                   0.0\n",
      "\"path\":                              /home/nafis/projects/def-kevinlb/nafis\n",
      "\"plugins\":                           None\n",
      "\"precision\":                         32\n",
      "\"prepare_data_per_node\":             True\n",
      "\"process_position\":                  0\n",
      "\"profiler\":                          None\n",
      "\"progress_bar_refresh_rate\":         None\n",
      "\"reload_dataloaders_every_epoch\":    False\n",
      "\"reload_dataloaders_every_n_epochs\": 0\n",
      "\"replace_sampler_ddp\":               True\n",
      "\"resume_from_checkpoint\":            None\n",
      "\"seed\":                              42\n",
      "\"stochastic_weight_avg\":             False\n",
      "\"sync_batchnorm\":                    False\n",
      "\"terminate_on_nan\":                  False\n",
      "\"test_path\":                         /project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321\n",
      "\"tokenizer_name_or_path\":            google/t5-v1_1-large\n",
      "\"tpu_cores\":                         None\n",
      "\"track_grad_norm\":                   -1\n",
      "\"train_batch_size\":                  16\n",
      "\"train_path\":                        /project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321\n",
      "\"truncated_bptt_steps\":              None\n",
      "\"val_check_interval\":                1.0\n",
      "\"val_path\":                          /project/6001769/nafis/MTA_FINAL_DATA/data_new/FinalData/12321\n",
      "\"warmup_steps\":                      0\n",
      "\"weight_decay\":                      0.0\n",
      "\"weights_save_path\":                 None\n",
      "\"weights_summary\":                   top\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a256e1ad1d441d9a4fea516207e518a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step end qwk : 0.2072230786170447 val-loss :  tensor(0.8824, device='cuda:0')\n",
      "Step end qwk : 0.026817804810616575 val-loss :  tensor(0.8802, device='cuda:0')\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val-epoch-loss': 0.8814563751220703,\n",
      " 'val_qwk': 0.1299065202474594,\n",
      " 'val_qwk_step': 0.1299065202474594,\n",
      " 'val_qwk_step_epoch': 0.1299065202474594}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a2217b945b49bb929409e9c0efc287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping\n",
      "Looping\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.19      0.22        26\n",
      "           2       0.00      0.00      0.00        27\n",
      "           3       0.24      0.33      0.28        52\n",
      "           4       0.45      0.57      0.50        97\n",
      "           5       0.08      0.05      0.06        22\n",
      "\n",
      "    accuracy                           0.35       224\n",
      "   macro avg       0.20      0.23      0.21       224\n",
      "weighted avg       0.29      0.35      0.31       224\n",
      "\n",
      "0.12705363085438048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nafis/jupyter_py3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f22c3bf00052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Purples'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'True labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moutputdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_test_confusion_matrix_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataSeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################Validate#####################################################\n",
    "dataset = Dataset(tokenizer, '', 'test',args=args)\n",
    "vloader = DataLoader(dataset, batch_size=128, num_workers=args.num_workers)\n",
    "trainer.validate(dataloaders=vloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = torch.load(checkpoint_callback.best_model_path)\n",
    "args = argparse.Namespace(**checkpoint)\n",
    "\n",
    "model = T5Custom(checkpoint[\"hyper_parameters\"])\n",
    "model = model.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "trainer.validate(model)\n",
    "\n",
    "\n",
    "\n",
    "model.model.eval()\n",
    "outputs = []\n",
    "targets = []\n",
    "for batch in tqdm(vloader):\n",
    "    outs = model.model.generate(input_ids=batch['source_ids'], \n",
    "                              attention_mask=batch['source_mask'], \n",
    "                              max_length=2)\n",
    "    dec = [tokenizer.decode(ids) for ids in outs]\n",
    "    target = [tokenizer.decode(ids) for ids in batch[\"target_ids\"]]\n",
    "    outputs.extend(dec)\n",
    "    targets.extend(target)\n",
    "    print (\"Looping\")\n",
    "outputs = [x.split(\" \")[-1] for x in outputs]\n",
    "targets = [x.split(\"<\")[0] for x in targets]\n",
    "print(metrics.classification_report(targets, outputs))\n",
    "y = [(int(x)) for x in targets]\n",
    "pred = [int(x) for x in outputs]\n",
    "\n",
    "# print (sklearn.metrics.cohen_kappa_score(o, t, weights=\"quadratic\"))\n",
    "# cm = metrics.confusion_matrix(t, o)\n",
    "# df_cm = pd.DataFrame(cm, index = [\"1\",\"2\",\"3\",\"4\",\"5\"], columns = [\"1\",\"2\",\"3\",\"4\",\"5\"])\n",
    "# plt.figure(figsize = (10,5))\n",
    "# sn.heatmap(df_cm, annot=True, cmap='Purples', fmt='g')\n",
    "CURPATH = os.getcwd()\n",
    "outputdir = \"./RunResults/\"+str(dataSeed)\n",
    "filename = str(seed)\n",
    "filepath = str(seed)+\".txt\"\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)\n",
    "\n",
    "test_qwk = sklearn.metrics.cohen_kappa_score(pred, y, weights=\"quadratic\")\n",
    "print (test_qwk)\n",
    "cm = confusion_matrix(y,pred)\n",
    "cm\n",
    "\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [\"1\",\"2\",\"3\",\"4\",\"5\"], columns = [\"1\",\"2\",\"3\",\"4\",\"5\"])\n",
    "plt.figure(figsize = (10,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(df_cm, annot=True, cmap='Purples', fmt='g',ax=ax)\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "plt.savefig(\"./\"+outputdir+\"/\"+filename+'_test_confusion_matrix_'+str(dataSeed)+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputdir+\"/\"+str(test_qwk)+\"_\"+str(dataSeed)+\"_\"+filepath, 'w') as f:\n",
    "    f.write(\"Validation QWK  : \"+ str(val_qwk))\n",
    "    f.write(\"Test QWK : \"+ str(test_qwk))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
